{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDU PRML 2024 Fall Assignment 1\n",
    "\n",
    "Name: 李增昊\n",
    "\n",
    "Student ID: 22307130108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions and complete the following exercises using PyTorch.\n",
    "\n",
    "## 1. Basic Operations of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "my_first_tensor = torch.ones((3,4))  # TODO: assign a tensor of shape (3, 4) with all elements equal to 1.0\n",
    "\n",
    "my_second_tensor = torch.randn((3, 4))  # TODO: assign a random tensor of shape (3, 4) with all elements sampled from a standard normal distribution\n",
    "\n",
    "their_matrix_product = torch.matmul(my_first_tensor, my_second_tensor.T)  # TODO: compute the matrix product of my_first_tensor and the transpose of my_second_tensor (There are multiple ways to do this. Just pick one you like.)\n",
    "\n",
    "some_meaningless_concatenation = torch.cat((my_first_tensor, my_second_tensor), dim=0)  # TODO: concatenate my_first_tensor and my_second_tensor along the first dimension. (Maybe you should check the documentation of torch.cat)\n",
    "\n",
    "some_meaningless_stack = torch.stack((my_first_tensor, my_first_tensor, my_first_tensor, my_first_tensor, my_first_tensor), dim=0) \n",
    "# TODO: stack 5 copies of my_first_tensor along a newly created dimension. (Maybe you should check the documentation of torch.stack)\n",
    "# What is the shape of some_meaningless_stack? Can you imagine the geometric interpretation of stacking 5 matrices of shape (3, 4) along the first dimension?\n",
    "\n",
    "#some_meaningless_stack的形状应该是[5, 3, 4]。这个操作就像是把同形状的张量在新的维度叠起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple logistic regression\n",
    "\n",
    "There are 4 core components in Pytorch training process: **model**, **loss function**, **optimizer** and **data loader**. In this part, we will implement a simple logistic regression model to illustrate them.\n",
    "\n",
    "### 2.1 Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear layer for logitstic regression\n",
    "\n",
    "class Linear(torch.nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# TODO: initialize the weight and bias of the linear layer.\n",
    "\t\tself.weight = torch.nn.Parameter(torch.randn(input_dim, output_dim)*0.01)\n",
    "\t\tself.bias = torch.nn.Parameter(torch.zeros(output_dim))\n",
    "  \n",
    "\tdef forward(self, x):\n",
    "\t\tpass\n",
    "\t\t# TODO: implement the forward function of a linear layer.\n",
    "\t\treturn x @ self.weight + self.bias\n",
    "\t\t# return torch.sigmoid(X@self.weight + self.bias)\n",
    "\n",
    "\n",
    "def loss_function(y_pred, y):\n",
    "    # TODO: implement the loss function of logistic regression.\n",
    "\treturn torch.nn.functional.binary_cross_entropy_with_logits(y_pred, y)\n",
    "\t#return torch.mean(y*torch.log(y_pred) + (1-y)*torch.log(1-y_pred))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data\n",
    "\n",
    "In real world, we usually have to deal with large-scale datasets. However, in this assignment, we will use synthetic data to illustrate the training process. The synthetic data is generated by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data for binary classification\n",
    "\n",
    "num_samples = 100\n",
    "num_features = 2\n",
    "\n",
    "x_0 = torch.randn(num_samples, num_features) + torch.tensor([2.0, 2.0])\n",
    "y_0 = torch.zeros(num_samples)\n",
    "\n",
    "x_1 = torch.randn(num_samples, num_features) + torch.tensor([-2.0, -2.0])\n",
    "y_1 = torch.ones(num_samples)\n",
    "\n",
    "x = torch.cat([x_0, x_1], dim=0)\n",
    "y = torch.cat([y_0, y_1], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset to feed into the model\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t# TODO: implement the __getitem__ function.\n",
    "\t\treturn self.x[index], self.y[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t# TODO: implement the __len__ function.\n",
    "\t\treturn len(self.x)\n",
    "\n",
    "\n",
    "dataset = MyDataset(x, y)\n",
    "dataloder = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Linear(num_features, 1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(my_model.parameters(), lr=0.01)  # TODO: initialize an optimizer of your choice.\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together\n",
    "\n",
    "Since this is just a toy experiment, we do not need validation.\n",
    "\n",
    "In the following code, we expect to see the training loss decreasing to 0.001 or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0016931958962231874\n",
      "epoch: 10, loss: 0.002506396733224392\n",
      "epoch: 20, loss: 0.00015973576228134334\n",
      "epoch: 30, loss: 0.001939267385751009\n",
      "epoch: 40, loss: 0.0020727734081447124\n",
      "epoch: 50, loss: 0.0005686646327376366\n",
      "epoch: 60, loss: 0.0007488030823878944\n",
      "epoch: 70, loss: 0.00013168402074370533\n",
      "epoch: 80, loss: 9.199530904879794e-05\n",
      "epoch: 90, loss: 0.0002735707676038146\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "for epoch in range(100):\n",
    "\tfor batch_x, batch_y in dataloder:\n",
    "\t\t# TODO: implement the training loop.\n",
    "\t\t#由于反向传播的默认行为是让梯度累加，所以归零\n",
    "\t\toptimizer.zero_grad() \n",
    "\t\ty_pred = my_model(batch_x)  \n",
    "\t\tloss = loss_function(y_pred.squeeze(), batch_y)  \n",
    "\t\tloss.backward()\n",
    "\t\t#限制梯度的最大值\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(my_model.parameters(), max_norm=0.3)\n",
    "\t\toptimizer.step()  \n",
    "\n",
    "  \n",
    "\tif epoch % 10 == 0:\n",
    "\t\tprint('epoch: {}, loss: {}'.format(epoch, loss.item()))\n",
    "  \n",
    "# save the model\n",
    "\n",
    "torch.save(my_model.state_dict(), 'my_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST Classification\n",
    "\n",
    "In this section, you will use PyTorch to implement a multi-layer perceptron (MLP) model for classifying handwritten digits using the MNIST dataset.\n",
    "\n",
    "\n",
    "1. Data Loading and Preprocessing:\n",
    "   - Utilize the `torchvision.datasets` module to load the MNIST dataset.\n",
    "   - Apply necessary transformations (like `ToTensor` and `Normalize`) to prepare the data for model training. These transformations ensure the data has the correct format and scales, helping with model convergence.\n",
    "   - Use a DataLoader with a suitable batch size to efficiently manage data feeding into the model.\n",
    "\n",
    "2. Architecture:\n",
    "   - Define a simple MLP model with PyTorch's `torch.nn.Module`. A suggested architecture is:\n",
    "     - An input layer that takes the flattened 28x28 pixel values (784 features).\n",
    "     - One or more hidden layers with ReLU activations for non-linearity.\n",
    "     - An output layer with softmax activation for multi-class classification.\n",
    "   - Make sure to initialize the model appropriately, especially if you're stacking multiple layers.\n",
    "\n",
    "3. Training:\n",
    "   - Set up an optimizer (like `Adam` or `SGD`) to minimize the model's error during training. You will also need a loss function, such as `CrossEntropyLoss`, which is well-suited for classification tasks.\n",
    "   - Write a training loop that performs the following steps:\n",
    "     - Forward pass: Feed batches through the model to obtain predictions.\n",
    "     - Compute the loss by comparing predictions with true labels.\n",
    "     - Backward pass: Calculate gradients for each model parameter.\n",
    "     - Update the model weights using the optimizer.\n",
    "   - Periodically log or print the training loss to track progress.\n",
    "\n",
    "4. Evaluation:\n",
    "   - After training, evaluate your model on the test set.\n",
    "   - Compute and print the accuracy metric, and optionally, create a confusion matrix to analyze classification errors.\n",
    "\n",
    "\n",
    "MNIST: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(in_features=7*7*64, out_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=128, out_features=10),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3136, out_features=128, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (12): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance of my MLP model\n",
    "my_second_model = MLP()\n",
    "my_second_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "LossFunction = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the optimizer\n",
    "MNIST_optimizer = torch.optim.Adam(my_second_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/10], Loss: 1.5648, Accuracy: 0.8958: 100%|██████████| 469/469 [00:41<00:00, 11.42step/s]\n",
      "[2/10], Loss: 1.4825, Accuracy: 0.9792: 100%|██████████| 469/469 [00:42<00:00, 10.94step/s]\n",
      "[3/10], Loss: 1.4818, Accuracy: 0.9792: 100%|██████████| 469/469 [00:42<00:00, 10.93step/s]\n",
      "[4/10], Loss: 1.4726, Accuracy: 0.9896: 100%|██████████| 469/469 [00:43<00:00, 10.85step/s]\n",
      "[5/10], Loss: 1.4714, Accuracy: 0.9896: 100%|██████████| 469/469 [00:43<00:00, 10.90step/s]\n",
      "[6/10], Loss: 1.4612, Accuracy: 1.0000: 100%|██████████| 469/469 [00:43<00:00, 10.90step/s]\n",
      "[7/10], Loss: 1.4729, Accuracy: 0.9896: 100%|██████████| 469/469 [00:43<00:00, 10.82step/s]\n",
      "[8/10], Loss: 1.4612, Accuracy: 1.0000: 100%|██████████| 469/469 [00:43<00:00, 10.89step/s]\n",
      "[9/10], Loss: 1.4612, Accuracy: 1.0000: 100%|██████████| 469/469 [00:43<00:00, 10.87step/s]\n",
      "[10/10], Loss: 1.4612, Accuracy: 1.0000: 100%|██████████| 469/469 [00:43<00:00, 10.88step/s]\n"
     ]
    }
   ],
   "source": [
    "# write the training loop\n",
    "MAX_EPOCH = 10\n",
    "for epoch in range(1, MAX_EPOCH + 1):\n",
    "    processBar = tqdm(train_loader, unit='step')\n",
    "    my_second_model.train(True)\n",
    "    for step, (train_img, labels) in enumerate(processBar):\n",
    "        train_img = train_img.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        my_second_model.zero_grad()\n",
    "        # feed the batch and get predictions\n",
    "        pred = my_second_model(train_img)\n",
    "        # compute the loss\n",
    "        loss = LossFunction(pred, labels)\n",
    "\n",
    "        pred_label = torch.argmax(pred, dim=1)\n",
    "        accuracy = torch.sum(pred_label == labels)/labels.shape[0]\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the model weights using optimizer\n",
    "        MNIST_optimizer.step()\n",
    "        processBar.set_description(\"[%d/%d], Loss: %.4f, Accuracy: %.4f\"%(epoch, MAX_EPOCH, loss.item(), accuracy.item()))\n",
    "    processBar.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test set: 1.4713, Accuracy on test set: 0.9788\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "my_second_model.train(False)\n",
    "\n",
    "total_loss = 0\n",
    "count = 0\n",
    "\n",
    "for test_img, labels in test_loader:\n",
    "    test_img = test_img.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    temp = my_second_model(test_img)\n",
    "    pred_label = torch.argmax(temp, dim=1)\n",
    "    loss = LossFunction(temp, labels)\n",
    "\n",
    "    total_loss +=loss\n",
    "    count += torch.sum(pred_label == labels)\n",
    "\n",
    "accuracy = count/(128*len(test_loader))\n",
    "final_loss = total_loss/(len(test_loader))\n",
    "\n",
    "print(\"Loss on test set: %.4f, Accuracy on test set: %.4f\"%(final_loss.item(), accuracy.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
